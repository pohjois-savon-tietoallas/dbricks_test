```{r intro-to-dtplyr, include = FALSE}
eval_dtplyr <- FALSE
# if(Sys.getenv("GLOBAL_EVAL") != "") eval_dtplyr <- Sys.getenv("GLOBAL_EVAL")
```


```{r, eval = eval_dtplyr, include = FALSE}
library(data.table)
library(dtplyr)
library(dplyr)
library(lobstr)
library(fs)
library(purrr)
```

# Introduction to `dtplyr`

## `dtplyr` basics
*Load data into R via `data.table`, and then wrap it with `dtplyr`*

1. Load the `data.table`, `dplyr`, `dtplyr`, `purrr` and `fs` libraries
    ```{r, eval = eval_dtplyr}
    library(data.table)
    library(dplyr)
    library(dtplyr)
    library(purrr)
    library(fs)
    ```

2. Read the **transactions.csv** file, from the **/usr/share/class/files** folder. Use the `fread()` function to load the data into a variable called `transactions`
    ```{r, eval = eval_dtplyr}
    transactions <- dir_ls("/usr/share/class/files", glob = "*.csv") %>%
       map(fread) %>%
       rbindlist()
    ```

3. Preview the data using `glimpse()`
    ```{r, eval = eval_dtplyr}
    
    ```

4. Use `lazy_dt()` to "wrap" the `transactions` variable into a new variable called `dt_transactions`
    ```{r, eval = eval_dtplyr}
    
    ```

5. View `dt_transactions` structure with `glimpse()`
    ```{r, eval = eval_dtplyr}
    
    ```

## Object sizes
*Confirm that `dtplyr` is not making copies of the original `data.table`*

1. Load the `lobstr` library
    ```{r, eval = eval_dtplyr}
    library(lobstr)
    ```

2. Use `obj_size()` to obtain `transactions`'s size in memory
    ```{r, eval = eval_dtplyr}
    
    ```

3. Use `obj_size()` to obtain `dt_transactions`'s size in memory
    ```{r, eval = eval_dtplyr}
    
    ```

4. Use `obj_size()` to obtain `dt_transactions` and `transactions` size in memory together
    ```{r, eval = eval_dtplyr}
    
    ```

## How `dtplyr` works
*Under the hood view of how `dtplyr` operates `data.table` objects*

1. Use `dplyr` verbs on top of `dt_transactions` to obtain the total sales by month
    ```{r, eval = eval_dtplyr}
    dt_transactions %>%
      group_by(date_month) %>%
      summarise(total_sales = sum(price))
    ```

2. Load the above code into a variable called `by_month`
    ```{r, eval = eval_dtplyr}
    
    ```

3. Use `show_query()` to see the `data.table` code that `by_month` actually runs
    ```{r, eval = eval_dtplyr}
    
    ```

4. Use `glimpse()` to view how `by_month`, instead of modifying the data, only adds steps that will later be executed by `data.table`
    ```{r, eval = eval_dtplyr}
    
    ```
    
5. Create a new column using `mutate()`
    ```{r, eval = eval_dtplyr}
    dt_transactions %>%
      mutate(new_field = price / 2)
    ```

6. Use `show_query()` to see the `copy()` command being used
    ```{r, eval = eval_dtplyr}
    
    ```

7. Check to confirm that the new column *did not* persist in `dt_transactions`
    ```{r, eval = eval_dtplyr}
    
    ```

8. Use `lazy_dt()` with the `immutable` argument set to `FALSE` to avoid the copy
    ```{r, eval = eval_dtplyr}
    m_transactions <- lazy_dt(copy(transactions), immutable = FALSE)
    ```

    ```{r, eval = eval_dtplyr}
    m_transactions
    ```

9. Create a `new_field` column in `m_transactions` using `mutate()`
    ```{r, eval = eval_dtplyr}
    m_transactions %>% 
      mutate(new_field = price / 2)
    ```

10. Use `show_query()` to see that `copy()` is no longer being used
    ```{r, eval = eval_dtplyr}
    
    ```

11. Inspect `m_transactions` to see that `new_field` has persisted
    ```{r, eval = eval_dtplyr}
    
    ```

## Working with `dtplyr`
*Learn data conversion and basic visualization techniques*

1. Use `as_tibble()` to convert the results of `by_month` into a `tibble`
    ```{r, eval = eval_dtplyr}
    by_month %>%
      as_tibble()
    ```

2. Load the `ggplot2` library
    ```{r, eval = eval_dtplyr}
    library(ggplot2)
    ```

3. Use `as_tibble()` to convert before creating a line plot 
    ```{r, eval = eval_dtplyr}
    by_month %>%
      
      ggplot() +
      geom_line(aes(date_month, total_sales))
    ```

## Pivot data
*Review a simple way to aggregate data faster, and then pivot it as a tibble*

1. Load the `tidyr` library
    ```{r, eval = eval_dtplyr}
    library(tidyr)
    ```

2. Group `db_transactions` by `date_month` and `date_day`, then aggregate `price` into `total_sales`
    ```{r, eval = eval_dtplyr}
    dt_transactions %>%
      group_by(date_month, date_day) %>% 
      summarise(total_sales = sum(price))
    ```

3. Copy the aggregation code above, **collect it into a `tibble`**, and then use `pivot_wider()` to make the `date_day` the column headers.
    ```{r, eval = eval_dtplyr}
    dt_transactions %>%
      group_by(date_month, date_day) %>% 
      summarise(total_sales = sum(price)) %>%
      
      pivot_wider(names_from = date_day, values_from = total_sales)
    ```

<!--chapter:end:dtplyr.Rmd-->

```{r, intro-to-sparklyr, include = FALSE}
eval_sparklyr <- FALSE
eval_sparkr <- FALSE
if(Sys.getenv("GLOBAL_EVAL") != "") eval_sparklyr <- Sys.getenv("GLOBAL_EVAL")
```

# Intro to `sparklyr`

*Testing Big Data workshop material in Databricks environment. RStudio is installed into the databricks cluster.*

```{r, eval = eval_sparklyr, include = FALSE}
library(dplyr)
library(sparklyr)
library(SparkR)
```

## New Spark session
*Learn to open a new Spark session*

1. New Spark session

    ```{r, eval = eval_sparklyr}
    library(SparkR)
    sparkR.session()
    ```

2. Load the `sparklyr` library
    ```{r, eval = eval_sparkr}
    library(sparklyr)
    ```

3. Use `spark_connect()` to create a new local Spark session
    ```{r, eval = eval_sparklyr}
    sc <- spark_connect(method = "databricks")
    ```
    
What does this error message mean?

    ```
    cannot create file '/usr/local/lib/R/site-library/sparklyr/java//sparklyr-2.4-2.11.jar', reason 'Permission denied'cannot create file '/usr/local/lib/R/site-library/sparklyr/java//sparklyr-2.2-2.11.jar', reason 'Permission denied'cannot create file '/usr/local/lib/R/site-library/sparklyr/java//sparklyr-2.1-2.11.jar', reason 'Permission denied'
    ```

4. Click on the `Spark` button to view the current Spark session's UI **NOT WORKING**

**Also no tables shown in Connections-tab**
  

5. Click on the `Log` button to see the message history **NOT WORKING**


## Data transfer
*Practice uploading data to Spark*

1. Load the `dplyr` library
    ```{r, eval = eval_sparklyr}
    library(dplyr)
    ```

2. Copy the `mtcars` dataset into the session
    ```{r, eval = eval_sparklyr}
    spark_mtcars <- copy_to(sc, mtcars, "my_mtcars", overwrite = T)
    ```

3. In the **Connections** pane, expande the `my_mtcars` table

4. Go to the Spark UI, note the new jobs **NOT WORKING**

5. In the UI, click the Storage button, note the new table **NOT WORKING**

6. Click on the **In-memory table my_mtcars** link **NOT WORKING**

## Spark and `dplyr`
*See how Spark handles `dplyr` commands*

```{r, eval = eval_sparklyr}
library(dplyr)
```

1. Run the following code snipett
    ```{r, eval = eval_sparklyr}
    spark_mtcars %>%
      dplyr::group_by(am) %>%
      dplyr::summarise(mpg_mean = mean(mpg, na.rm = TRUE))
    ```

2. Go to the Spark UI and click the **SQL** button  **NOT WORKING**

3. Click on the top item inside the **Completed Queries** table **NOT WORKING**

4. At the bottom of the diagram, expand **Details**  **NOT WORKING**

## Feature transformers
*Introduction to how Spark Feature Transformers can be called from R*

1. Use `ft_binarizer()` to create a new column, called `over_20`, that indicates if that row's `mpg` value is over or under 20MPG
    ```{r, eval = eval_sparklyr}
    spark_mtcars %>%
      ft_binarizer("mpg", "over_20", 20)
    ```


2. Pipe the code into `count()` to see how the data splits between the two values
    ```{r, eval = eval_sparklyr}
    spark_mtcars %>%
      ft_binarizer("mpg", "over_20", 20) %>%
      dplyr::count(over_20)
    ```


3. Start a new code chunk. This time use `ft_quantile_discretizer()` to create a new column called `mpg_quantile`
    ```{r, eval = eval_sparklyr}
    spark_mtcars %>%
      ft_quantile_discretizer("mpg", "mpg_quantile")
    ```

4. Add the `num_buckets` argument to `ft_quantile_discretizer()`, set its value to 5
    ```{r, eval = eval_sparklyr}
    spark_mtcars %>%
      ft_quantile_discretizer("mpg", "mpg_quantile", num_buckets = 5)
    ```

5. 1. Pipe the code into `count()` to see how the data splits between the quantiles
    ```{r, eval = eval_sparklyr}
    spark_mtcars %>%
      ft_quantile_discretizer("mpg", "mpg_quantile", num_buckets = 5) %>%
      dplyr::count(mpg_quantile)
    ```

## Models
*Introduce Spark ML models by running a couple of them in R*

1. Use `ml_kmeans()` to run a model based on the following formula: `wt ~ mpg`.  Assign the results to a variable called `k_mtcars`
    ```{r, eval = eval_sparklyr}
    k_mtcars <- spark_mtcars %>%
      ml_kmeans(wt ~ mpg)
    ```

2. Use `k_mtcars$summary` to view the results of the model.  Pull the cluster sizes by using `...$cluster_sizes()`
    ```{r, eval = eval_sparklyr}
    k_mtcars$summary$cluster_sizes()
    ```

3. Start a new code chunk. This time use `ml_linear_regression()` to produce a Linear Regression model of the same formula used in the previous model. Assign the results to a variable called `lr_mtcars`
    ```{r, eval = eval_sparklyr}
    lr_mtcars <- spark_mtcars %>% 
      ml_linear_regression(wt ~ mpg)
    ```

4. Use `summary()` to view the results of the model
    ```{r, eval = eval_sparklyr}
    summary(lr_mtcars)
    ```

<!--chapter:end:sparklyr.Rmd-->

```{r, textmining, include = FALSE}
eval_mining <- FALSE
if(Sys.getenv("GLOBAL_EVAL") != "") eval_mining <- Sys.getenv("GLOBAL_EVAL")
```

```{r, eval = eval_mining, include = FALSE}
library(wordcloud2)
library(sparklyr)
library(dplyr)
```

# Text mining with `sparklyr`

For this example, there are two files that will be analyzed.  They are both the full works of Sir Arthur Conan Doyle and Mark Twain.  The files were downloaded from the [Gutenberg Project](https://www.gutenberg.org/) site via the `gutenbergr` package.  Intentionally, no data cleanup was done to the files prior to this analysis.  See the appendix below to see how the data was downloaded and prepared.

```{r, eval = eval_mining}
readLines("../class/data/arthur_doyle.txt", 30)
```



## Data Import
*Read the book data into Spark*

1. Load the `sparklyr` library
    ```{r, eval = eval_mining}
    library(sparklyr)
    ```

2. Open a Spark session
    ```{r, eval = eval_mining}
    sc <- spark_connect(master = "local")
    ```

3. Use the `spark_read_text()` function to read the **mark_twain.txt** file, assign it to a variable called `twain`
    ```{r, eval = eval_mining}
    twain <- spark_read_text(sc, "twain", "../class/books/mark_twain.txt") 
    ```

4. Use the `spark_read_text()` function to read the **arthur_doyle.txt** file, assign it to a variable called `doyle`
    ```{r, eval = eval_mining}
    doyle <- spark_read_text(sc, "doyle", "../class/books/arthur_doyle.txt") 
    ```


## Tidying data
*Prepare the data for analysis*

1. Load the `dplyr` library
    ```{r}
    library(dplyr)
    ```

2. Add a column to `twain` named `author` with a value of "twain".  Assign it to a new variable called `twain_id`
    ```{r, eval = eval_mining}
    twain_id <- twain %>% 
      mutate(author = "twain")
    ```

3. Add a column to `doyle` named `author` with a value of "doyle".  Assign it to a new variable called `doyle_id`
    ```{r, eval = eval_mining}
    doyle_id <- doyle %>%
      mutate(author = "doyle")
    ```

4. Use `sdf_bind_rows()` to append the two files together in a variable called `both`
    ```{r, eval = eval_mining}
    both <- doyle_id %>%
      sdf_bind_rows(twain_id) 
    ```

5. Preview `both`
    ```{r, eval = eval_mining}
    both
    ```

6. Filter out empty lines into a variable called `all_lines`
    ```{r, eval = eval_mining}
    all_lines <-  both %>%
      filter(nchar(line) > 0)
    ```

7. Use Hive's *regexp_replace* to remove punctuation, assign it to the same `all_lines` variable
    ```{r, eval = eval_mining}
    all_lines <- all_lines %>%
      mutate(line = regexp_replace(line, "[_\"\'():;,.!?\\-]", " "))
    ```

## Transform the data
*Use feature transformers to make additional preparations*

1. Use `ft_tokenizer()` to separate each word. in the line.  Set the `output_col` to "word_list". Assign to a variable called `word_list`
    ```{r, eval = eval_mining}
    word_list <- all_lines %>%
      ft_tokenizer(
        input_col = "line",
        output_col = "word_list"
        )
    ```

2. Remove "stop words" with the `ft_stop_words_remover()` transformer. Set the `output_col` to "wo_stop_words". Assign to a variable called `wo_stop`
    ```{r, eval = eval_mining}
    wo_stop <- word_list %>%
      ft_stop_words_remover(
        input_col = "word_list",
        output_col = "wo_stop_words"
        )
    ```

3. Un-nest the tokens inside *wo_stop_words* using `explode()`.  Assign to a variable called `exploded`
    ```{r, eval = eval_mining}
    exploded <- wo_stop %>%
      mutate(word = explode(wo_stop_words))
    ```

4. Select the *word* and *author* columns, and remove any word with less than 3 characters. Assign to `all_words`
    ```{r, eval = eval_mining}
    all_words <- exploded %>%
      select(word, author) %>%
      filter(nchar(word) > 2)
    ```

5. Cache the `all_words` variable using `compute()`  
    ```{r, eval = eval_mining}
    all_words <- all_words %>%
      compute("all_words")
    ```

## Data Exploration
*Used word clouds to explore the data*

1. Create a variable with the word count by author, name it `word_count`
    ```{r, eval = eval_mining}
    word_count <- all_words %>%
      count(author, word, sort = TRUE) 
    ```

2. Filter `word_cout` to only retain "twain", assign it to `twain_most`
    ```{r, eval = eval_mining}
    twain_most <- word_count %>%
      filter(author == "twain")
    ```

3. Use `wordcloud` to visualize the top 50 words used by Twain
    ```{r, eval = eval_mining}
    twain_most %>%
      head(50) %>%
      collect() %>%
      with(wordcloud::wordcloud(
        word, 
        n,
        colors = c("#999999", "#E69F00", "#56B4E9","#56B4E9"))
        )
    ```

4. Filter `word_cout` to only retain "doyle", assign it to `doyle_most`
    ```{r, eval = eval_mining}
    doyle_most <- word_count %>%
      filter(author == "doyle")
    ```

5. Used `wordcloud` to visualize the top 50 words used by Doyle that have more than 5 characters
    ```{r, eval = eval_mining}
    doyle_most %>%
      filter(nchar(word) > 5) %>%
      head(50) %>%
      collect() %>%
      with(wordcloud::wordcloud(
        word, 
        n,
        colors = c("#999999", "#E69F00", "#56B4E9","#56B4E9")
        ))
    ```

6. Use `anti_join()` to figure out which words are used by Doyle but not Twain. Order the results by number of words.
    ```{r, eval = eval_mining}
    doyle_unique <- doyle_most %>%
      anti_join(twain_most, by = "word") %>%
      arrange(desc(n)) 
    ```

7. Use `wordcloud` to visualize top 50 records in the previous step
    ```{r, eval = eval_mining}
    doyle_unique %>%
      head(50) %>%
      collect() %>%
      with(wordcloud::wordcloud(
        word, 
        n,
        colors = c("#999999", "#E69F00", "#56B4E9","#56B4E9"))
        )
    ```

8. Find out how many times Twain used the word "sherlock"
    ```{r, eval = eval_mining}
    all_words %>%
      filter(author == "twain", word == "sherlock") %>%
      tally()
    ```

9. Against the `twain` variable, use Hive's *instr* and *lower* to make all ever word lower cap, and then look for "sherlock" in the line
    ```{r, eval = eval_mining}
    twain %>%
      mutate(line = lower(line)) %>%
      filter(instr(line, "sherlock") > 0) %>%
      pull(line)
    ```

10. Close Spark session
    ```{r, eval = eval_mining}
    spark_disconnect(sc)
    ```

Most of these lines are in a short story by Mark Twain called [A Double Barrelled Detective Story](https://www.gutenberg.org/files/3180/3180-h/3180-h.htm#link2H_4_0008). As per the [Wikipedia](https://en.wikipedia.org/wiki/A_Double_Barrelled_Detective_Story) page about this story, this is a satire by Twain on the mystery novel genre, published in 1902.



## Appendix

```{r, eval=FALSE}
library(gutenbergr)

gutenberg_works()  %>%
  filter(author == "Twain, Mark") %>%
  pull(gutenberg_id) %>%
  gutenberg_download() %>%
  pull(text) %>%
  writeLines("mark_twain.txt")
```

<!--chapter:end:sparklyr_text-mining.Rmd-->

