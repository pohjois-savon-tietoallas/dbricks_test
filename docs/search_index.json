[
["databricks-connect.html", "1 Databricks Connect 1.1 Background 1.2 Cluster setup 1.3 Client setup 1.4 Get connection values 1.5 Configure connection settings 1.6 Setting up RStudio 1.7 Troubleshoot", " 1 Databricks Connect Description: Instructions for installing and configuring Databricks connect Author: Eric Le Tortorec, Jani Miettinen Date: 2020-03-19 R version 3.6.2 (2019-12-12) 1.1 Background These instructions are for installing and configuring Databricks Connect in order to access Databricks from your local computer. Sorry Windows users, the installation is more complicated for you and will require admin privileges. NOTE Once everything has been installed and is running the results that are returned from your analyses will be downloaded onto your computer. Therefore it is absolutely forbidden to analyse KUH data using Databricks Connect! 1.2 Cluster setup Create a Spark cluster with Databricks Runtime version 5.5 or 6.1 and above. If you are using Databricks Runtime 5.3 or below (unsupported), click the Spark tab and add the following Spark conf: spark.databricks.service.server.enabled true 1.3 Client setup 1.3.1 Install Java Download and install Java 8, which is the only Java version supported by Spark. NOTE you have to create an account at Oracle in order to download the installation file. Windows users. Java will try to install itself into the Program Files-directory. Path has a space in it, which will cause problems. Install it in C:\\\\Java instead. You might also have to set the JAVA_HOME variable, more information here. 1.3.2 Install Python Download and install Python. It is easiest to install Python via Anaconda. Anaconda includes a environment management system called conda, which should be used to create a new environment inside which you can install the packages needed for Databricks connect. The minor version of your client Python installation must be the same as the minor Python version of your Azure Databricks cluster (2.7, 3.5, 3.6, or 3.7). Databricks Runtime 5.x has Python 3.5, Databricks Runtime 5.x ML has Python 3.6, and Databricks Runtime 6.x and Databricks Runtime 6.x ML have Python 3.7. Windows users. Anaconda comes with a command line interface called Anaconda Powershell Prompt. Use this to write the code blocks below, unless otherwise instructed. Mac users. You can use the terminal to write the commands. Create the environment with: conda create --name dbconnect python=3.7 Where dbconnect is the name of the environment and python=3.7 specifies the version of Python you want to install. Activate the environment with: conda activate dbconnect Once inside the new environment configure the environment for Databricks connect. If pyspark has been installed in your environment you will need to uninstall it: pip uninstall pyspark Then install the Databricks Connect client: pip install -U databricks-connect==6.2.* Where databricks-connect==6.2.* is the Databricks Runtime version 1.4 Get connection values In order to connect to Databricks running on Azure, you will need specific information about the cluster you will be connecting to: Databricks host Cluster ID Organisation ID Port User token This information can be found from the URL of your cluster. E.g.: https://northeurope.azuredatabricks.net/?o=2208xxxxxxxxxxxx#/setting/clusters/0212-yyyyyy-yyyyyyyy/ In this case the Databricks host is https://northeurope.azuredatabricks.net (The host will change when the KUH cloud environment is moved to West Europe.) The cluster ID is: 0212-yyyyyy-yyyyyyyy The organisation ID is the part after ?o= in this case: 2208xxxxxxxxxxxx The port will be 15001 by default Finally, a user token needs to be created. Click on the user profile icon in the upper right had corner of the Databricks website, click on User Settings and then the Access Tokens tab. From there you can create a token, give some information about it in the form of a comment, and spaecify a lifetime. Copy the created token and store it securely. NOTE This is like a password, treat it as such! 1.5 Configure connection settings Within the Python environment you created before run the following in order to supply the configuration values: databricks-connect configure You will be presented with a license to accept and then fields in which to supply the values gathered above. You should now be able to test the connection with: databricks-connect test NOTE I was not able to get this to work even though the connection worked through RStudio. I read about the same experience elsewhere as well. Mac gave Warning of Java version WARNING: Java versions &gt;8 are not supported by this SDK. 1.6 Setting up RStudio Start off by downloading and unpacking Spark on your computer. Make sure the Hadoop version of Spark is 2.7. Unpack in e.g. C:\\\\Users\\\\username\\\\AppData\\\\Local\\\\Apache\\\\Spark. Make a note of where you unpacked spark. Windows users Spark will not work on Windows without WinUtils, which contains Windows binaries for Hadoop. This will require admin privileges. More information can be found here. Also, make sure that your installation path (includes your username) for Spark does not have a space in it! If it does you can get around it by modifying the Windows registry, which can really screw up your Windows installation! Do this at your own risk! More information can be found here. Then run the following command to get the path where the pyspark Java archive files are: databricks-connect get-jar-dir This will return a path like this Windows: C:\\\\users\\\\username\\\\appdata\\\\local\\\\continuum\\\\anaconda3\\\\envs\\\\dbconnect\\\\lib\\\\site-packages\\\\pyspark/jars Mac: /usr/local/lib/python3.7/site-packages/pyspark/jars Copy the file path of one directory above the JAR directory file path. For example C:\\\\users\\\\username\\\\appdata\\\\local\\\\continuum\\\\anaconda3\\\\envs\\\\dbconnect\\\\lib\\\\site-packages\\\\pyspark Now switch over to RStudio. First, install SparkR with command install.packages(&quot;SparkR&quot;). Then copy the two file directory paths and use them to tell SparkR where to look for Spark and pyspark. On a Windows computer backslashes need to be escaped, therefore the double backslashes in the path names. On a Mac computer the path will have front slashes. In Windows: library(dplyr) library(SparkR, lib.loc = .libPaths(c(file.path(&quot;C:\\\\Users\\\\username\\\\AppData\\\\Local\\\\Apache\\\\Spark\\\\Cache\\\\spark-2.4.3-bin-hadoop2.7&quot;, &quot;R&quot;, &quot;lib&quot;), .libPaths()))) Sys.setenv(SPARK_HOME = &quot;c:\\\\users\\\\ericle\\\\appdata\\\\local\\\\continuum\\\\anaconda3\\\\envs\\\\dbconnect_6_2\\\\lib\\\\site-packages\\\\pyspark&quot;) In Mac: library(dplyr) library(SparkR, lib.loc = .libPaths(c(file.path(&quot;~/AppData/spark-2.4.5-bin-hadoop2.7/&quot;, &#39;R&#39;, &#39;lib&#39;), .libPaths()))) Sys.setenv(SPARK_HOME = &quot;/usr/local/lib/python3.7/site-packages/pyspark/&quot;) You should now be able to initiate a Spark session and start running SparkR commands. If your cluster is not running it should start automatically (this might take some minutes). SparkR::sparkR.session() df &lt;- as.DataFrame(faithful) df1 &lt;- dapply(df, function(x) { x }, schema(df)) collect(df1) nba_player_stats &lt;- read.df(source = &quot;csv&quot;, path = &quot;dbfs:/FileStore/tables/nbaplayerstats1519_2-d5cfb.csv&quot;, header=&quot;true&quot;, inferSchema = &quot;true&quot;, sep = &#39;;&#39;) head(nba_player_stats) You can stop the Spark session but the cluster will not shut down until the preset autotermination time, or until you terminate it manually. SparkR::sparkR.session.stop() 1.7 Troubleshoot 1.7.1 Mac: Error starting a spark session In mac starting a new spark session with command SparkR::sparkR.session() might give you an error in console: Java version 8 is required for this package; found version: 11.0.1 Check installed java version in terminal: ls -l /Library/Java/JavaVirtualMachines Set Java version directory in R before running SparkR session command: Sys.setenv(JAVA_HOME = &quot;/Library/Java/JavaVirtualMachines/jdk1.8.0_241.jdk/Contents/Home&quot;) "],
["text-mining-with-sparklyr.html", "2 Text mining with sparklyr 2.1 Data Importg 2.2 Tidying data 2.3 Transform the data 2.4 Data Exploration 2.5 Appendix", " 2 Text mining with sparklyr For this example, there are two files that will be analyzed. They are both the full works of Sir Arthur Conan Doyle and Mark Twain. The files were downloaded from the Gutenberg Project site via the gutenbergr package. Intentionally, no data cleanup was done to the files prior to this analysis. See the appendix below to see how the data was downloaded and prepared. readLines(&quot;./data/arthur_doyle.txt&quot;, 30) 2.1 Data Importg Read the book data into Spark Moving data files to dbfs # fs::file_copy(&quot;./data/arthur_doyle.txt&quot;, &quot;/dbfs/arthur_doyle.txt&quot;) # fs::file_copy(&quot;./data/mark_twain.txt&quot;, &quot;/dbfs/mark_twain.txt&quot;, overwrite = T) system(&quot;cp ./data/arthur_doyle.txt /dbfs/arthur_doyle.txt&quot;) system(&quot;cp ./data/mark_twain.txt /dbfs/mark_twain.txt&quot;) list.files(&quot;/dbfs/&quot;) Load SparkR and open new session library(SparkR) sparkR.session() Load the sparklyr library and open a Spark session library(sparklyr) sc &lt;- spark_connect(method = &quot;databricks&quot;) Use the spark_read_text() function to read the mark_twain.txt file, assign it to a variable called twain twain &lt;- spark_read_text(sc, &quot;twain&quot;, &quot;mark_twain.txt&quot;) Use the spark_read_text() function to read the arthur_doyle.txt file, assign it to a variable called doyle doyle &lt;- spark_read_text(sc, &quot;doyle&quot;, &quot;arthur_doyle.txt&quot;) 2.2 Tidying data Prepare the data for analysis Load the dplyr library library(dplyr) Add a column to twain named author with a value of “twain”. Assign it to a new variable called twain_id twain_id &lt;- twain %&gt;% mutate(author = &quot;twain&quot;) Add a column to doyle named author with a value of “doyle”. Assign it to a new variable called doyle_id doyle_id &lt;- doyle %&gt;% mutate(author = &quot;doyle&quot;) Use sdf_bind_rows() to append the two files together in a variable called both both &lt;- doyle_id %&gt;% sdf_bind_rows(twain_id) Preview both both Filter out empty lines into a variable called all_lines all_lines &lt;- both %&gt;% filter(nchar(line) &gt; 0) Use Hive’s regexp_replace to remove punctuation, assign it to the same all_lines variable all_lines &lt;- all_lines %&gt;% mutate(line = regexp_replace(line, &quot;[_\\&quot;\\&#39;():;,.!?\\\\-]&quot;, &quot; &quot;)) 2.3 Transform the data Use feature transformers to make additional preparations Use ft_tokenizer() to separate each word. in the line. Set the output_col to “word_list”. Assign to a variable called word_list word_list &lt;- all_lines %&gt;% ft_tokenizer( input_col = &quot;line&quot;, output_col = &quot;word_list&quot; ) Remove “stop words” with the ft_stop_words_remover() transformer. Set the output_col to “wo_stop_words”. Assign to a variable called wo_stop wo_stop &lt;- word_list %&gt;% ft_stop_words_remover( input_col = &quot;word_list&quot;, output_col = &quot;wo_stop_words&quot; ) Un-nest the tokens inside wo_stop_words using explode(). Assign to a variable called exploded exploded &lt;- wo_stop %&gt;% mutate(word = explode(wo_stop_words)) Select the word and author columns, and remove any word with less than 3 characters. Assign to all_words all_words &lt;- exploded %&gt;% select(word, author) %&gt;% filter(nchar(word) &gt; 2) Cache the all_words variable using compute() all_words &lt;- all_words %&gt;% compute(&quot;all_words&quot;) 2.4 Data Exploration Used word clouds to explore the data Create a variable with the word count by author, name it word_count word_count &lt;- all_words %&gt;% count(author, word, sort = TRUE) Filter word_cout to only retain “twain”, assign it to twain_most twain_most &lt;- word_count %&gt;% filter(author == &quot;twain&quot;) Use wordcloud to visualize the top 50 words used by Twain twain_most %&gt;% head(50) %&gt;% collect() %&gt;% with(wordcloud::wordcloud( word, n, colors = c(&quot;#999999&quot;, &quot;#E69F00&quot;, &quot;#56B4E9&quot;,&quot;#56B4E9&quot;)) ) Filter word_cout to only retain “doyle”, assign it to doyle_most doyle_most &lt;- word_count %&gt;% filter(author == &quot;doyle&quot;) Used wordcloud to visualize the top 50 words used by Doyle that have more than 5 characters doyle_most %&gt;% filter(nchar(word) &gt; 5) %&gt;% head(50) %&gt;% collect() %&gt;% with(wordcloud::wordcloud( word, n, colors = c(&quot;#999999&quot;, &quot;#E69F00&quot;, &quot;#56B4E9&quot;,&quot;#56B4E9&quot;) )) Use anti_join() to figure out which words are used by Doyle but not Twain. Order the results by number of words. doyle_unique &lt;- doyle_most %&gt;% anti_join(twain_most, by = &quot;word&quot;) %&gt;% arrange(desc(n)) Use wordcloud to visualize top 50 records in the previous step doyle_unique %&gt;% head(50) %&gt;% collect() %&gt;% with(wordcloud::wordcloud( word, n, colors = c(&quot;#999999&quot;, &quot;#E69F00&quot;, &quot;#56B4E9&quot;,&quot;#56B4E9&quot;)) ) Find out how many times Twain used the word “sherlock” all_words %&gt;% filter(author == &quot;twain&quot;, word == &quot;sherlock&quot;) %&gt;% tally() Against the twain variable, use Hive’s instr and lower to make all ever word lower cap, and then look for “sherlock” in the line twain %&gt;% mutate(line = lower(line)) %&gt;% filter(instr(line, &quot;sherlock&quot;) &gt; 0) %&gt;% pull(line) Close Spark session spark_disconnect(sc) Most of these lines are in a short story by Mark Twain called A Double Barrelled Detective Story. As per the Wikipedia page about this story, this is a satire by Twain on the mystery novel genre, published in 1902. 2.5 Appendix How to download books library(gutenbergr) gutenberg_works() %&gt;% filter(author == &quot;Twain, Mark&quot;) %&gt;% pull(gutenberg_id) %&gt;% gutenberg_download() %&gt;% pull(text) %&gt;% writeLines(&quot;mark_twain.txt&quot;) "],
["intro-to-sparklyr.html", "3 Intro to sparklyr 3.1 New Spark session 3.2 Data transfer 3.3 Spark and dplyr 3.4 Feature transformers 3.5 Models", " 3 Intro to sparklyr Testing Big Data workshop material in Databricks environment. RStudio is installed into the databricks cluster. 3.1 New Spark session Learn to open a new Spark session New Spark session library(SparkR) sparkR.session() Load the sparklyr library library(sparklyr) Use spark_connect() to create a new local Spark session sc &lt;- spark_connect(method = &quot;databricks&quot;) What does this error message mean? ``` cannot create file &#39;/usr/local/lib/R/site-library/sparklyr/java//sparklyr-2.4-2.11.jar&#39;, reason &#39;Permission denied&#39;cannot create file &#39;/usr/local/lib/R/site-library/sparklyr/java//sparklyr-2.2-2.11.jar&#39;, reason &#39;Permission denied&#39;cannot create file &#39;/usr/local/lib/R/site-library/sparklyr/java//sparklyr-2.1-2.11.jar&#39;, reason &#39;Permission denied&#39; ``` Click on the Spark button to view the current Spark session’s UI NOT WORKING Also no tables shown in Connections-tab Click on the Log button to see the message history NOT WORKING 3.2 Data transfer Practice uploading data to Spark Load the dplyr library library(dplyr) Copy the mtcars dataset into the session spark_mtcars &lt;- copy_to(sc, mtcars, &quot;my_mtcars&quot;) In the Connections pane, expande the my_mtcars table Go to the Spark UI, note the new jobs NOT WORKING In the UI, click the Storage button, note the new table NOT WORKING Click on the In-memory table my_mtcars link NOT WORKING 3.3 Spark and dplyr See how Spark handles dplyr commands Run the following code snipett spark_mtcars %&gt;% group_by(am) %&gt;% summarise(mpg_mean = mean(mpg, na.rm = TRUE)) Go to the Spark UI and click the SQL button NOT WORKING Click on the top item inside the Completed Queries table NOT WORKING At the bottom of the diagram, expand Details NOT WORKING 3.4 Feature transformers Introduction to how Spark Feature Transformers can be called from R Use ft_binarizer() to create a new column, called over_20, that indicates if that row’s mpg value is over or under 20MPG spark_mtcars %&gt;% ft_binarizer(&quot;mpg&quot;, &quot;over_20&quot;, 20) Pipe the code into count() to see how the data splits between the two values spark_mtcars %&gt;% ft_binarizer(&quot;mpg&quot;, &quot;over_20&quot;, 20) %&gt;% dplyr::count(over_20) Start a new code chunk. This time use ft_quantile_discretizer() to create a new column called mpg_quantile spark_mtcars %&gt;% ft_quantile_discretizer(&quot;mpg&quot;, &quot;mpg_quantile&quot;) Add the num_buckets argument to ft_quantile_discretizer(), set its value to 5 spark_mtcars %&gt;% ft_quantile_discretizer(&quot;mpg&quot;, &quot;mpg_quantile&quot;, num_buckets = 5) Pipe the code into count() to see how the data splits between the quantiles spark_mtcars %&gt;% ft_quantile_discretizer(&quot;mpg&quot;, &quot;mpg_quantile&quot;, num_buckets = 5) %&gt;% dplyr::count(mpg_quantile) 3.5 Models Introduce Spark ML models by running a couple of them in R Use ml_kmeans() to run a model based on the following formula: wt ~ mpg. Assign the results to a variable called k_mtcars k_mtcars &lt;- spark_mtcars %&gt;% ml_kmeans(wt ~ mpg) Use k_mtcars$summary to view the results of the model. Pull the cluster sizes by using ...$cluster_sizes() k_mtcars$summary$cluster_sizes() Start a new code chunk. This time use ml_linear_regression() to produce a Linear Regression model of the same formula used in the previous model. Assign the results to a variable called lr_mtcars lr_mtcars &lt;- spark_mtcars %&gt;% ml_linear_regression(wt ~ mpg) Use summary() to view the results of the model summary(lr_mtcars) "],
["testing-databricks-connect-on-desktop-rstudio-.html", "4 Testing databricks connect on desktop RStudio. 4.1 Connect to databricks 4.2 Test 4.3 Datasets 4.4 Stop 4.5 Notes", " 4 Testing databricks connect on desktop RStudio. Setup first databricks connect. 4.1 Connect to databricks Create connect to demodata databricks library(dplyr) library(SparkR, lib.loc = .libPaths(c(file.path(&quot;~/AppData/spark-2.4.5-bin-hadoop2.7/&quot;, &#39;R&#39;, &#39;lib&#39;), .libPaths()))) Sys.setenv(SPARK_HOME = &quot;/usr/local/lib/python3.7/site-packages/pyspark/&quot;) Sys.setenv(JAVA_HOME = &quot;/Library/Java/JavaVirtualMachines/jdk1.8.0_241.jdk/Contents/Home&quot;) SparkR::sparkR.session() 4.2 Test Test connection with faithful dataset df &lt;- as.DataFrame(faithful) df1 &lt;- dapply(df, function(x) { x }, schema(df)) collect(df1) 4.3 Datasets Test datasets in project area # Function for viewing data View &lt;- function(data,n=1001,...) { display(collect(head(data,n,...))) } ## Datasets persons &lt;- read.parquet(&#39;/mnt/demodata/persons.parquet&#39;) care_reg &lt;- read.parquet(&#39;/mnt/demodata/care_reg.parquet&#39;) # drugs &lt;- read.parquet(&#39;/mnt/demodata/drugs.parquet&#39;) # mort &lt;- read.parquet(&#39;/mnt/demodata/mort.parquet&#39;) # srr &lt;- read.parquet(&#39;/mnt/demodata/srr.parquet&#39;) # inst_reg &lt;- read.parquet(&#39;/mnt/demodata/inst_reg.parquet&#39;)# ## EDA str(care_reg) summary(care_reg) 4.4 Stop You can stop the Spark session but the cluster will not shut down until the preset autotermination time, or until you terminate it manually. SparkR::sparkR.session.stop() 4.5 Notes "]
]
