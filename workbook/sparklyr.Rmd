```{r, intro-to-sparklyr, include = FALSE}
eval_sparklyr <- FALSE
eval_sparkr <- FALSE
if(Sys.getenv("GLOBAL_EVAL") != "") eval_sparklyr <- Sys.getenv("GLOBAL_EVAL")
```

# Intro to `sparklyr`

*Testing Big Data workshop material in Databricks environment. RStudio is installed into the databricks cluster.*

```{r, eval = eval_sparklyr, include = FALSE}
library(dplyr)
library(sparklyr)
library(SparkR)
```

## New Spark session
*Learn to open a new Spark session*

1. New Spark session

    ```{r, eval = eval_sparklyr}
    library(SparkR)
    sparkR.session()
    ```

2. Load the `sparklyr` library
    ```{r, eval = eval_sparkr}
    library(sparklyr)
    ```

3. Use `spark_connect()` to create a new local Spark session
    ```{r, eval = eval_sparklyr}
    sc <- spark_connect(method = "databricks")
    ```
    
What does this error message mean?

    ```
    cannot create file '/usr/local/lib/R/site-library/sparklyr/java//sparklyr-2.4-2.11.jar', reason 'Permission denied'cannot create file '/usr/local/lib/R/site-library/sparklyr/java//sparklyr-2.2-2.11.jar', reason 'Permission denied'cannot create file '/usr/local/lib/R/site-library/sparklyr/java//sparklyr-2.1-2.11.jar', reason 'Permission denied'
    ```

4. Click on the `Spark` button to view the current Spark session's UI **NOT WORKING**

**Also no tables shown in Connections-tab**
  

5. Click on the `Log` button to see the message history **NOT WORKING**


## Data transfer
*Practice uploading data to Spark*

1. Load the `dplyr` library
    ```{r, eval = eval_sparklyr}
    library(dplyr)
    ```

2. Copy the `mtcars` dataset into the session
    ```{r, eval = eval_sparklyr}
    spark_mtcars <- copy_to(sc, mtcars, "my_mtcars", overwrite = T)
    ```

3. In the **Connections** pane, expande the `my_mtcars` table

4. Go to the Spark UI, note the new jobs **NOT WORKING**

5. In the UI, click the Storage button, note the new table **NOT WORKING**

6. Click on the **In-memory table my_mtcars** link **NOT WORKING**

## Spark and `dplyr`
*See how Spark handles `dplyr` commands*

```{r, eval = eval_sparklyr}
library(dplyr)
```

1. Run the following code snipett
    ```{r, eval = eval_sparklyr}
    spark_mtcars %>%
      dplyr::group_by(am) %>%
      dplyr::summarise(mpg_mean = mean(mpg, na.rm = TRUE))
    ```

2. Go to the Spark UI and click the **SQL** button  **NOT WORKING**

3. Click on the top item inside the **Completed Queries** table **NOT WORKING**

4. At the bottom of the diagram, expand **Details**  **NOT WORKING**

## Feature transformers
*Introduction to how Spark Feature Transformers can be called from R*

1. Use `ft_binarizer()` to create a new column, called `over_20`, that indicates if that row's `mpg` value is over or under 20MPG
    ```{r, eval = eval_sparklyr}
    spark_mtcars %>%
      ft_binarizer("mpg", "over_20", 20)
    ```


2. Pipe the code into `count()` to see how the data splits between the two values
    ```{r, eval = eval_sparklyr}
    spark_mtcars %>%
      ft_binarizer("mpg", "over_20", 20) %>%
      dplyr::count(over_20)
    ```


3. Start a new code chunk. This time use `ft_quantile_discretizer()` to create a new column called `mpg_quantile`
    ```{r, eval = eval_sparklyr}
    spark_mtcars %>%
      ft_quantile_discretizer("mpg", "mpg_quantile")
    ```

4. Add the `num_buckets` argument to `ft_quantile_discretizer()`, set its value to 5
    ```{r, eval = eval_sparklyr}
    spark_mtcars %>%
      ft_quantile_discretizer("mpg", "mpg_quantile", num_buckets = 5)
    ```

5. 1. Pipe the code into `count()` to see how the data splits between the quantiles
    ```{r, eval = eval_sparklyr}
    spark_mtcars %>%
      ft_quantile_discretizer("mpg", "mpg_quantile", num_buckets = 5) %>%
      dplyr::count(mpg_quantile)
    ```

## Models
*Introduce Spark ML models by running a couple of them in R*

1. Use `ml_kmeans()` to run a model based on the following formula: `wt ~ mpg`.  Assign the results to a variable called `k_mtcars`
    ```{r, eval = eval_sparklyr}
    k_mtcars <- spark_mtcars %>%
      ml_kmeans(wt ~ mpg)
    ```

2. Use `k_mtcars$summary` to view the results of the model.  Pull the cluster sizes by using `...$cluster_sizes()`
    ```{r, eval = eval_sparklyr}
    k_mtcars$summary$cluster_sizes()
    ```

3. Start a new code chunk. This time use `ml_linear_regression()` to produce a Linear Regression model of the same formula used in the previous model. Assign the results to a variable called `lr_mtcars`
    ```{r, eval = eval_sparklyr}
    lr_mtcars <- spark_mtcars %>% 
      ml_linear_regression(wt ~ mpg)
    ```

4. Use `summary()` to view the results of the model
    ```{r, eval = eval_sparklyr}
    summary(lr_mtcars)
    ```
